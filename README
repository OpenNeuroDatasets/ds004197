Bimodal dataset on Inner Speech

Code available: https://github.com/LTU-Machine-Learning/Inner_Speech_EEG_FMRI 
Publication available: TODO 

Abstract:
Inner speech recognition is a challenging research area within Brain Computer Interfaces (BCI), which could give a `voice' to patients that have no ability to speak or move. Multimodal datasets of brain data enable fusion of neuroimaging modalities with complementary properties, such as the high spatial resolution of functional Magnetic Resonance Imaging (fMRI) and temporal resolution of Electroencephalography (EEG). This may hold promise for inner-speech decoding.
This paper presents the first publicly available bimodal EEG and fMRI dataset acquired non-simultaneously during inner speech production. EEG and fMRI data are acquired from four subjects as they performed an inner speech task for words belonging to a social or numerical category. There are in total 8 word stimuli, each with 40 trials, resulting in 320 trials in each modality for each subject.

The dataset consists of 1280 trials in each modality (EEG, FMRI). 
The stimuli contain 8 words,  selected from 2 different categories (social, numeric):
Social: child, daughter, father, wife
Numeric: four, three, ten, six

There are 4 subjects in total: sub01, sub02, sub03, sub05. Initially, there were 5 participants, however, sub04 data 
was rejected due to high fluctuations. Details of valid data are available in the file participants.tsv.
